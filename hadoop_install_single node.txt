
Single Node Installation

sudo apt-get update

ssh-keygen 

cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys

----------**downloading and installing java (Hadoop requires a working Java 1.6+)**--------

#sudo apt-get install openjdk-7-jdk

sudo apt install openjdk-8-jre-headless -y

-----------------------------**downloading hadoop**------------------------

#wget http://apache.mirror.gtcomm.net/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz

wget https://archive.apache.org/dist/hadoop/core/hadoop-1.2.1/hadoop-1.2.1.tar.gz

tar -xzvf hadoop-1.2.1.tar.gz

sudo mv hadoop-1.2.1 /usr/local/hadoop

----------------- Centos7 ----------------- 
#imran chaush
#+918237267747


nano ~/.bashrc

export HADOOP_PREFIX=/usr/local/hadoop/
export PATH=$PATH:$HADOOP_PREFIX/bin
export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
export PATH=$PATH:$JAVA_HOME

nano /usr/local/hadoop/conf/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/jre-1.8.0-openjdk
export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true


hadoop-daemon.sh start namenode
hadoop-daemon.sh start DataNode
hadoop-daemon.sh start secondarynamenode
hadoop-daemon.sh start TaskTracker
hadoop-daemon.sh start JobTracker


hadoop-daemon.sh stop datanode
----------------- ----------------- ----------------- 
---------------------------**configuring bashrc linux env setup**---------------



nano ~/.bashrc

export HADOOP_PREFIX=/usr/local/hadoop/
export PATH=$PATH:$HADOOP_PREFIX/bin
#export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME

----**setting hadoop env**------
nano /usr/local/hadoop/conf/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true

-------**configuring xml's**------
nano /usr/local/hadoop/conf/core-site.xml
<property>
<name>fs.default.name</name>
<value>hdfs://localhost:9000</value>
</property>

<property>
<name>hadoop.tmp.dir</name>
<value>/usr/local/hadoop/tmp</value>
</property>

nano /usr/local/hadoop/conf/hdfs-site.xml
<property>
<name>dfs.replication</name>
<value>1</value>
</property>

nano /usr/local/hadoop/conf/mapred-site.xml
<property>
<name>mapred.job.tracker</name>
<value>hdfs://localhost:9001</value>
</property>

----------**making tmp dir**---------

mkdir /usr/local/hadoop/tmp

-------------**exec bash**-------


exec bash

----------------**formatting hadoop namenode**--------------

hadoop namenode -format

---------------**starting hadoop services(daemons)**-------------------
start-all.sh 

sudo apt install openjdk-8-jdk-headless -y
jps




100.26.29.51:50090 
100.26.29.51:50075
100.26.29.51:50070
100.26.29.51:50030
100.26.29.51:50060



nano  wordcountfile
this is test file

hadoop fs -mkdir /user/hadoop

hadoop fs -copyFromLocal wordcountfile /user/hadoop

hadoop jar /usr/local/hadoop/hadoop-examples-1.2.1.jar wordcount /user/hadoop/wordcountfile /user/hadoop/result

hadoop fs -ls /user/hadoop/result/
hadoop fs -copyToLocal /user/hadoop/result/part-r-00000 .
cat part-r-00000

